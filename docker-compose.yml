version: '3.8'

services:
  know-thyself:
    image: jolleyboy/know-thyself:latest
    build:
      context: .
      dockerfile: Dockerfile
    # Use host network to access Ollama on localhost
    network_mode: host
    environment:
      - NODE_ENV=production
      # Optional: Only needed if using Claude API (not needed for Ollama/local LLM)
      # NEVER hardcode API keys here - use environment variables or .env file
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      # Ollama URL for local LLM support
      - OLLAMA_BASE_URL=http://localhost:11434
      - DATA_DIR=/app/data
      - PORT=3000
    volumes:
      - know-thyself-data:/app/data
    restart: unless-stopped

volumes:
  know-thyself-data:
    driver: local
