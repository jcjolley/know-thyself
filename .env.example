# =============================================================================
# Know Thyself - Environment Configuration
# =============================================================================

# LLM Provider Configuration
# --------------------------
# You can use either Claude API (Anthropic) or a local LLM via Ollama.
# At least one must be configured.

# Option 1: Claude API (cloud-based)
# Get your API key from https://console.anthropic.com/
# IMPORTANT: Never commit real API keys to git!
ANTHROPIC_API_KEY=

# Option 2: Ollama (local LLM - no API key needed)
# Install Ollama from https://ollama.ai and run: ollama pull llama3.2
# Default URL works if Ollama is running locally
OLLAMA_BASE_URL=http://localhost:11434

# Server Configuration (optional)
# -------------------------------
# PORT=3000
# DATA_DIR=/app/data
